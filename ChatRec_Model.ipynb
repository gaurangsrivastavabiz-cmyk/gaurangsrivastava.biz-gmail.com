{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc52e16-0415-489b-a3f7-277902e256a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "USE_CUDA = True\n",
    "\n",
    "# Paths - edit if needed\n",
    "DATA_DIR = Path('/Desktop/Dataset')\n",
    "USER_A_CSV = DATA_DIR / 'userA_chats.csv'\n",
    "USER_B_CSV = DATA_DIR / 'userB_chats.csv'\n",
    "# optional local xlsx if you downloaded it manually\n",
    "CONV_XLSX = Path('conversationfile.xlsx')\n",
    "\n",
    "OUTPUT_DIR = Path('./chatrec_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model choice (local HuggingFace name; must be available offline in cache)\n",
    "# Options: 'gpt2', 'distilgpt2', 'gpt2-medium' (choose small if CPU-only)\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "\n",
    "# Data handling\n",
    "MAX_CONTEXT_TOKENS = 512   # tokens to take from A's history (keep smaller for speed)\n",
    "MAX_REPLY_TOKENS = 128     # max tokens to generate for reply\n",
    "MAX_TOTAL_TOKENS = 512 + 128  # must be <= model config max_position_embeddings\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4  # per device\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Generation parameters (for inference)\n",
    "GEN_MAX_LENGTH = 128\n",
    "GEN_TEMPERATURE = 0.8\n",
    "GEN_TOP_K = 50\n",
    "GEN_TOP_P = 0.9\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "\n",
    "# Misc\n",
    "USE_CUDA = True\n",
    "DEVICE = 'cuda' if (USE_CUDA and (os.environ.get('CUDA_VISIBLE_DEVICES') or torch.cuda.is_available())) else 'cpu'\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d18134a-3b04-4827-a4c7-a4c95fa43641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heyja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b61a956890>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# nltk BLEU\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# simple rouge-l implementation provided below (no external package required)\n",
    "\n",
    "# reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3314d87-e4ad-4f36-9d23-f1245d34ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# ROUGE-L (LCS based)\n",
    "def _lcs(a: List[str], b: List[str]) -> int:\n",
    "    # classic DP for longest common subsequence\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        for j in range(m - 1, -1, -1):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i][j] = 1 + dp[i + 1][j + 1]\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])\n",
    "    return dp[0][0]\n",
    "\n",
    "\n",
    "def rouge_l_score(reference: str, hypothesis: str) -> float:\n",
    "    r_tokens = nltk.word_tokenize(reference.lower())\n",
    "    h_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "    if len(r_tokens) == 0 or len(h_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs = _lcs(r_tokens, h_tokens)\n",
    "    prec = lcs / len(h_tokens)\n",
    "    rec = lcs / len(r_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    beta = 1.2\n",
    "    f_score = ((1 + beta*2) * prec * rec) / (rec + beta*2 * prec + 1e-12)\n",
    "    return f_score\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    return ' '.join(s.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dedab7dc-d607-4181-a9bd-cf3e53003b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userA rows: 11 userB rows: 11\n",
      "Constructed conversations dataframe shape: (22, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sender</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-07 10:15:12</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Hey, did you see the client's feedback on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-07 10:15:45</td>\n",
       "      <td>A</td>\n",
       "      <td>\"Just saw it. They want a lot of changes to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-07 10:16:05</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Yeah, that's what I was thinking. It's a big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-07 10:16:38</td>\n",
       "      <td>A</td>\n",
       "      <td>\"I'll start on the revisions. Can you update t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-07 10:17:01</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Will do. I'll block out the rest of the week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-10-07 10:20:19</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Any plans for Saturday?\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp sender  \\\n",
       "0 2025-10-07 10:15:12      B   \n",
       "1 2025-10-07 10:15:45      A   \n",
       "2 2025-10-07 10:16:05      B   \n",
       "3 2025-10-07 10:16:38      A   \n",
       "4 2025-10-07 10:17:01      B   \n",
       "5 2025-10-07 10:20:19      B   \n",
       "\n",
       "                                             message  \n",
       "0  \"Hey, did you see the client's feedback on the...  \n",
       "1  \"Just saw it. They want a lot of changes to th...  \n",
       "2  \"Yeah, that's what I was thinking. It's a big ...  \n",
       "3  \"I'll start on the revisions. Can you update t...  \n",
       "4  \"Will do. I'll block out the rest of the week ...  \n",
       "5                          \"Any plans for Saturday?\"  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_chat_file(path: Path, user_label: str) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"Warning: {path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path)\n",
    "    # heuristics to find message column\n",
    "    msg_cols = [c for c in df.columns if c.lower() in ('message', 'text', 'msg', 'content')]\n",
    "    if len(msg_cols) == 0:\n",
    "        # fallback: try last column\n",
    "        msg_col = df.columns[-1]\n",
    "    else:\n",
    "        msg_col = msg_cols[0]\n",
    "    df = df.rename(columns={msg_col: 'message'})\n",
    "    # add user label\n",
    "    df['sender'] = user_label\n",
    "    # ensure timestamp if present\n",
    "    ts_cols = [c for c in df.columns if 'time' in c.lower() or 'timestamp' in c.lower()]\n",
    "    if len(ts_cols) > 0:\n",
    "        df['timestamp'] = pd.to_datetime(df[ts_cols[0]], errors='coerce')\n",
    "    else:\n",
    "        # create artificial timestamp using index (keep ordering)\n",
    "        df['timestamp'] = pd.NaT\n",
    "    # conversation id if exists\n",
    "    if 'conversation_id' not in df.columns and 'conv_id' in df.columns:\n",
    "        df = df.rename(columns={'conv_id': 'conversation_id'})\n",
    "    return df[['conversation_id'] if 'conversation_id' in df.columns else [] + ['timestamp', 'sender', 'message']]\n",
    "\n",
    "# read both\n",
    "userA = read_chat_file(\"C:/Users/heyja/Downloads/userA_chats.csv\", 'A')\n",
    "userB = read_chat_file('C:/Users/heyja/Downloads/userB_chats.csv', 'B')\n",
    "\n",
    "print('userA rows:', len(userA), 'userB rows:', len(userB))\n",
    "\n",
    "# if both files empty and xlsx exists, try reading it (user may have downloaded the sheet locally)\n",
    "if len(userA) == 0 and len(userB) == 0 and CONV_XLSX.exists():\n",
    "    try:\n",
    "        conv = pd.read_excel('C:/Users/heyja/Downloads/conversationfile.xlsx')\n",
    "        print('Read conversationfile.xlsx shape', conv.shape)\n",
    "        # try to detect sender column\n",
    "        if 'sender' in conv.columns and 'message' in conv.columns:\n",
    "            conversations = conv[['conversation_id'] if 'conversation_id' in conv.columns else [] + ['timestamp', 'sender', 'message']]\n",
    "        else:\n",
    "            # assume alternating two-person: tag alternately\n",
    "            conv = conv.reset_index(drop=True)\n",
    "            conv['sender'] = ['A' if i % 2 == 0 else 'B' for i in range(len(conv))]\n",
    "            conv['message'] = conv.iloc[:,0].astype(str)\n",
    "            conversations = conv[['timestamp', 'sender', 'message']]\n",
    "    except Exception as e:\n",
    "        print('Failed to read xlsx:', e)\n",
    "        conversations = pd.DataFrame()\n",
    "else:\n",
    "    # combine and sort\n",
    "    combined = pd.concat([userA, userB], ignore_index=True, sort=False)\n",
    "    # if timestamp present use it; else keep original ordering\n",
    "    if combined['timestamp'].notna().any():\n",
    "        combined = combined.sort_values('timestamp').reset_index(drop=True)\n",
    "    else:\n",
    "        combined = combined.reset_index(drop=True)\n",
    "    conversations = combined[['timestamp', 'sender', 'message']]\n",
    "\n",
    "print('Constructed conversations dataframe shape:', conversations.shape)\n",
    "conversations.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad889677-0d31-440a-83fa-be3de4ed95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples built: 10\n",
      "Train examples: 10 Val examples: 10\n"
     ]
    }
   ],
   "source": [
    "def build_examples_from_timeline(df: pd.DataFrame, max_history_msgs=10):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    examples = []\n",
    "    # iterate over timeline, find pattern B -> A\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.loc[i-1]\n",
    "        cur = df.loc[i]\n",
    "        if prev['sender'] == 'B' and cur['sender'] == 'A':\n",
    "            # collect previous A messages before position i-1 (history)\n",
    "            history_msgs = []\n",
    "            j = i-2\n",
    "            while j >= 0 and len(history_msgs) < max_history_msgs:\n",
    "                if df.loc[j]['sender'] == 'A':\n",
    "                    history_msgs.append(clean_text(str(df.loc[j]['message'])))\n",
    "                j -= 1\n",
    "            history_msgs = list(reversed(history_msgs))\n",
    "            examples.append({\n",
    "                'a_history': ' \\n '.join(history_msgs),\n",
    "                'b_message': clean_text(str(prev['message'])),\n",
    "                'a_reply': clean_text(str(cur['message'])),\n",
    "            })\n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "examples = build_examples_from_timeline(conversations, max_history_msgs=8)\n",
    "print('Examples built:', len(examples))\n",
    "examples.sample(3)\n",
    "\n",
    "# if zero examples, try a fallback: pair alternate messages\n",
    "if len(examples) == 0 and len(conversations) > 1:\n",
    "    fallback = []\n",
    "    for i in range(0, len(conversations) - 1, 2):\n",
    "        a_msg = conversations.loc[i]['message']\n",
    "        b_msg = conversations.loc[i+1]['message']\n",
    "        fallback.append({'a_history': '', 'b_message': str(b_msg), 'a_reply': str(a_msg)})\n",
    "    examples = pd.DataFrame(fallback)\n",
    "    print('Fallback pairing created examples:', len(examples))\n",
    "\n",
    "# split train/val\n",
    "train_df, val_df = train_test_split(examples, test_size=0.1, random_state=SEED) if len(examples) > 10 else (examples, examples)\n",
    "print('Train examples:', len(train_df), 'Val examples:', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d75c851d-e1aa-4b4c-8a0c-3f1f9bf417ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\heyja\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length (config): 1024\n"
     ]
    }
   ],
   "source": [
    "print('Loading tokenizer and model:', MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# ensure tokenizer has a pad token (gpt2 doesn't by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# resize embeddings if we added pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(DEVICE)\n",
    "\n",
    "print('Model max length (config):', getattr(model.config, 'n_positions', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb60af68-67bd-4fd8-8495-6c5ddcadf935",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatReplyDataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m: input_ids_padded, \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: attention_mask, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: labels_padded}\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# build dataset objects\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m train_dataset = \u001b[43mChatReplyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_df) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     66\u001b[39m eval_dataset = ChatReplyDataset(val_df, tokenizer) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_df) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTrain dataset size:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: ChatReplyDataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "PREFIX_TEMPLATE = \"A_HISTORY: {a_history} \\n B: {b_message} \\n A:\"\n",
    "\n",
    "class ChatReplyDataset(Dataset):\n",
    "    def _init_(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_context_tokens=MAX_CONTEXT_TOKENS, max_reply_tokens=MAX_REPLY_TOKENS):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.max_reply_tokens = max_reply_tokens\n",
    "        for _, row in df.iterrows():\n",
    "            a_hist = row.get('a_history', '') or ''\n",
    "            b_msg = row.get('b_message', '') or ''\n",
    "            a_reply = row.get('a_reply', '') or ''\n",
    "            prompt = PREFIX_TEMPLATE.format(a_history=a_hist, b_message=b_msg)\n",
    "            # full text = prompt + ' ' + reply\n",
    "            full_text = prompt + ' ' + a_reply\n",
    "            # tokenize without truncation first (we will manually enforce)\n",
    "            enc = tokenizer(full_text, return_tensors='pt', truncation=False)\n",
    "            input_ids = enc['input_ids'][0]\n",
    "            # find where reply starts (by tokenizing prompt)\n",
    "            prompt_enc = tokenizer(prompt, return_tensors='pt', truncation=False)\n",
    "            prompt_len = prompt_enc['input_ids'].size(1)\n",
    "            # enforce length constraints from the right: keep last max_context_tokens of prompt tokens\n",
    "            # and the reply truncated to max_reply_tokens\n",
    "            # If prompt too long, truncate prompt tokens from the left (keep recent context)\n",
    "            # We'll compute token segments:\n",
    "            reply_ids = input_ids[prompt_len:]\n",
    "            prompt_ids = input_ids[:prompt_len]\n",
    "            # truncate reply\n",
    "            if reply_ids.size(0) > max_reply_tokens:\n",
    "                reply_ids = reply_ids[:max_reply_tokens]\n",
    "            # truncate prompt to keep recent tokens\n",
    "            if prompt_ids.size(0) > max_context_tokens:\n",
    "                prompt_ids = prompt_ids[-max_context_tokens:]\n",
    "            # compose final sequence\n",
    "            final_ids = torch.cat([prompt_ids, reply_ids], dim=0)\n",
    "            # labels: -100 for prompt tokens, actual ids for reply tokens\n",
    "            labels = final_ids.clone()\n",
    "            labels[:prompt_ids.size(0)] = -100\n",
    "            # pad to model max length if necessary\n",
    "            max_len = self.tokenizer.model_max_length\n",
    "            if final_ids.size(0) > max_len:\n",
    "                final_ids = final_ids[-max_len:]\n",
    "                labels = labels[-max_len:]\n",
    "            self.examples.append({'input_ids': final_ids, 'labels': labels})\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        item = self.examples[idx]\n",
    "        return {k: v.clone().detach() for k, v in item.items()}\n",
    "\n",
    "# collate function to pad batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    attention_mask = (input_ids_padded != tokenizer.pad_token_id).long()\n",
    "    return {'input_ids': input_ids_padded, 'attention_mask': attention_mask, 'labels': labels_padded}\n",
    "\n",
    "# build dataset objects\n",
    "train_dataset = ChatReplyDataset(train_df, tokenizer) if len(train_df) > 0 else None\n",
    "eval_dataset = ChatReplyDataset(val_df, tokenizer) if len(val_df) > 0 else None\n",
    "print('Train dataset size:', len(train_dataset) if train_dataset else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86446e94-ceb9-4320-a2df-a3f0de70d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset is not None and len(train_dataset) > 0:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR / 'model'),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        evaluation_strategy='epoch' if eval_dataset is not None and len(eval_dataset) > 0 else 'no',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        fp16=(DEVICE == 'cuda'),\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset if eval_dataset is not None and len(eval_dataset) > 0 else None,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    # quick sanity check run (optional):\n",
    "    print('Starting training...')\n",
    "    trainer.train()\n",
    "    print('Training finished')\n",
    "\n",
    "    # save\n",
    "    model_save_path = OUTPUT_DIR / 'model' / 'final'\n",
    "    model.save_pretrained(str(model_save_path))\n",
    "    tokenizer.save_pretrained(str(model_save_path))\n",
    "    print('Model and tokenizer saved to', model_save_path)\n",
    "else:\n",
    "    print('No training data found; skipping training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76bc36-7412-4b96-8b4a-7bfefe545e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(model, tokenizer, a_history: str, b_message: str, gen_kwargs=None):\n",
    "    if gen_kwargs is None:\n",
    "        gen_kwargs = {}\n",
    "    prompt = PREFIX_TEMPLATE.format(a_history=a_history, b_message=b_message) + ' '\n",
    "    # ensure prompt fits model\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length - 1).input_ids.to(DEVICE)\n",
    "    # generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=min(tokenizer.model_max_length, input_ids.shape[1] + GEN_MAX_LENGTH),\n",
    "        do_sample=True,\n",
    "        temperature=GEN_TEMPERATURE,\n",
    "        top_k=GEN_TOP_K,\n",
    "        top_p=GEN_TOP_P,\n",
    "        num_return_sequences=NUM_RETURN_SEQUENCES,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    # decode the completed sequence and return only the part after the prompt\n",
    "    generated = []\n",
    "    for out in outputs:\n",
    "        out_text = tokenizer.decode(out[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        generated.append(out_text.strip())\n",
    "    return generated\n",
    "\n",
    "# Evaluate on validation set (if exists)\n",
    "if eval_dataset is not None and len(eval_dataset) > 0:\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    for idx, row in val_df.reset_index(drop=True).iterrows():\n",
    "        a_hist = row['a_history']\n",
    "        b_msg = row['b_message']\n",
    "        ref = row['a_reply']\n",
    "        gen = generate_reply(model, tokenizer, a_hist, b_msg)[0]\n",
    "        refs.append(ref)\n",
    "        hyps.append(gen)\n",
    "        # BLEU (sentence-level small smoothing)\n",
    "        try:\n",
    "            bleu = sentence_bleu([nltk.word_tokenize(ref.lower())], nltk.word_tokenize(gen.lower()), smoothing_function=SmoothingFunction().method1)\n",
    "        except Exception:\n",
    "            bleu = 0.0\n",
    "        bleu_scores.append(bleu)\n",
    "        rouge_scores.append(rouge_l_score(ref, gen))\n",
    "    avg_bleu = float(np.mean(bleu_scores))\n",
    "    avg_rouge_l = float(np.mean(rouge_scores))\n",
    "    print(f'Validation BLEU: {avg_bleu:.4f}  ROUGE-L: {avg_rouge_l:.4f}')\n",
    "    # compute perplexity via trainer.evaluate if possible\n",
    "    try:\n",
    "        eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "        eval_loss = eval_metrics.get('eval_loss')\n",
    "        perplexity = math.exp(eval_loss) if eval_loss is not None and eval_loss < 100 else float('inf')\n",
    "        print('Eval loss:', eval_loss, 'Perplexity:', perplexity)\n",
    "    except Exception as e:\n",
    "        print('Could not compute perplexity via trainer:', e)\n",
    "\n",
    "    # save a small csv with results\n",
    "    res_df = pd.DataFrame({'reference': refs, 'generated': hyps, 'bleu': bleu_scores, 'rouge_l': rouge_scores})\n",
    "    res_df.to_csv(OUTPUT_DIR / 'val_generation_results.csv', index=False)\n",
    "    print('Saved generation results to', OUTPUT_DIR / 'val_generation_results.csv')\n",
    "else:\n",
    "    print('No eval dataset - skipping generation evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfcb28-b1fa-435e-a586-0b0cfa6c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatRecPredictor:\n",
    "    def _init_(self, model_dir: str):\n",
    "        self.model_dir = model_dir\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "        self.device = DEVICE\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, a_history: str, b_message: str, num_return_sequences=1):\n",
    "        prompt = PREFIX_TEMPLATE.format(a_history=a_history, b_message=b_message) + ' '\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=self.tokenizer.model_max_length - 1).input_ids.to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=min(self.tokenizer.model_max_length, input_ids.shape[1] + GEN_MAX_LENGTH),\n",
    "            do_sample=True,\n",
    "            temperature=GEN_TEMPERATURE,\n",
    "            top_k=GEN_TOP_K,\n",
    "            top_p=GEN_TOP_P,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        results = []\n",
    "        for out in outputs:\n",
    "            out_text = self.tokenizer.decode(out[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            results.append(out_text.strip())\n",
    "        return results\n",
    "\n",
    "# if we saved model above, create predictor and joblib dump\n",
    "model_dir = str((OUTPUT_DIR / 'model' / 'final'))\n",
    "if os.path.exists(model_dir):\n",
    "    predictor = ChatRecPredictor(model_dir)\n",
    "    joblib.dump(predictor, OUTPUT_DIR / 'Model.joblib')\n",
    "    print('Saved Model.joblib to', OUTPUT_DIR / 'Model.joblib')\n",
    "else:\n",
    "    # if model not saved (training skipped), save a small metadata joblib instructing user how to load\n",
    "    meta = {'note': 'No trained model found. Run training cells first and save model to ./chatrec_output/model/final'}\n",
    "    joblib.dump(meta, OUTPUT_DIR / 'Model.joblib')\n",
    "    print('Saved placeholder Model.joblib to', OUTPUT_DIR / 'Model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b6149-4de0-4132-bb93-ab0141e9546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "if (OUTPUT_DIR / 'Model.joblib').exists():\n",
    "    loaded = joblib.load(OUTPUT_DIR / 'Model.joblib')\n",
    "    if isinstance(loaded, ChatRecPredictor) or hasattr(loaded, 'predict'):\n",
    "        print('Loaded predictor from joblib. Testing with a sample:')\n",
    "        sample_a_hist = 'I was thinking of going for a run in the morning. Sleep was short yesterday.'\n",
    "        sample_b = 'Do you want to join the soccer match tonight?'\n",
    "        preds = loaded.predict(sample_a_hist, sample_b, num_return_sequences=2)\n",
    "        pprint(preds)\n",
    "    else:\n",
    "        print('Model.joblib is a placeholder; train and save model first')\n",
    "else:\n",
    "    print('No Model.joblib found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
