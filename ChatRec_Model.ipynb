{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc52e16-0415-489b-a3f7-277902e256a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "USE_CUDA = True\n",
    "\n",
    "# Paths - edit if needed\n",
    "DATA_DIR = Path('/Desktop/Dataset')\n",
    "USER_A_CSV = DATA_DIR / 'userA_chats.csv'\n",
    "USER_B_CSV = DATA_DIR / 'userB_chats.csv'\n",
    "# optional local xlsx if you downloaded it manually\n",
    "CONV_XLSX = Path('conversationfile.xlsx')\n",
    "\n",
    "OUTPUT_DIR = Path('./chatrec_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model choice (local HuggingFace name; must be available offline in cache)\n",
    "# Options: 'gpt2', 'distilgpt2', 'gpt2-medium' (choose small if CPU-only)\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "\n",
    "# Data handling\n",
    "MAX_CONTEXT_TOKENS = 512   # tokens to take from A's history (keep smaller for speed)\n",
    "MAX_REPLY_TOKENS = 128     # max tokens to generate for reply\n",
    "MAX_TOTAL_TOKENS = 512 + 128  # must be <= model config max_position_embeddings\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4  # per device\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Generation parameters (for inference)\n",
    "GEN_MAX_LENGTH = 128\n",
    "GEN_TEMPERATURE = 0.8\n",
    "GEN_TOP_K = 50\n",
    "GEN_TOP_P = 0.9\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "\n",
    "# Misc\n",
    "USE_CUDA = True\n",
    "DEVICE = 'cuda' if (USE_CUDA and (os.environ.get('CUDA_VISIBLE_DEVICES') or torch.cuda.is_available())) else 'cpu'\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d18134a-3b04-4827-a4c7-a4c95fa43641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heyja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1913abf40d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# nltk BLEU\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# simple rouge-l implementation provided below (no external package required)\n",
    "\n",
    "# reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3314d87-e4ad-4f36-9d23-f1245d34ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# ROUGE-L (LCS based)\n",
    "def _lcs(a: List[str], b: List[str]) -> int:\n",
    "    # classic DP for longest common subsequence\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        for j in range(m - 1, -1, -1):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i][j] = 1 + dp[i + 1][j + 1]\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])\n",
    "    return dp[0][0]\n",
    "\n",
    "\n",
    "def rouge_l_score(reference: str, hypothesis: str) -> float:\n",
    "    r_tokens = nltk.word_tokenize(reference.lower())\n",
    "    h_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "    if len(r_tokens) == 0 or len(h_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs = _lcs(r_tokens, h_tokens)\n",
    "    prec = lcs / len(h_tokens)\n",
    "    rec = lcs / len(r_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    beta = 1.2\n",
    "    f_score = ((1 + beta*2) * prec * rec) / (rec + beta*2 * prec + 1e-12)\n",
    "    return f_score\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    return ' '.join(s.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedab7dc-d607-4181-a9bd-cf3e53003b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userA rows: 11 userB rows: 11\n",
      "Constructed conversations dataframe shape: (22, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sender</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-07 10:15:12</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Hey, did you see the client's feedback on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-07 10:15:45</td>\n",
       "      <td>A</td>\n",
       "      <td>\"Just saw it. They want a lot of changes to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-07 10:16:05</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Yeah, that's what I was thinking. It's a big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-07 10:16:38</td>\n",
       "      <td>A</td>\n",
       "      <td>\"I'll start on the revisions. Can you update t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-07 10:17:01</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Will do. I'll block out the rest of the week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-10-07 10:20:19</td>\n",
       "      <td>B</td>\n",
       "      <td>\"Any plans for Saturday?\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp sender  \\\n",
       "0 2025-10-07 10:15:12      B   \n",
       "1 2025-10-07 10:15:45      A   \n",
       "2 2025-10-07 10:16:05      B   \n",
       "3 2025-10-07 10:16:38      A   \n",
       "4 2025-10-07 10:17:01      B   \n",
       "5 2025-10-07 10:20:19      B   \n",
       "\n",
       "                                             message  \n",
       "0  \"Hey, did you see the client's feedback on the...  \n",
       "1  \"Just saw it. They want a lot of changes to th...  \n",
       "2  \"Yeah, that's what I was thinking. It's a big ...  \n",
       "3  \"I'll start on the revisions. Can you update t...  \n",
       "4  \"Will do. I'll block out the rest of the week ...  \n",
       "5                          \"Any plans for Saturday?\"  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_chat_file(path: Path, user_label: str) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"Warning: {path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path)\n",
    "    # heuristics to find message column\n",
    "    msg_cols = [c for c in df.columns if c.lower() in ('message', 'text', 'msg', 'content')]\n",
    "    if len(msg_cols) == 0:\n",
    "        # fallback: try last column\n",
    "        msg_col = df.columns[-1]\n",
    "    else:\n",
    "        msg_col = msg_cols[0]\n",
    "    df = df.rename(columns={msg_col: 'message'})\n",
    "    # add user label\n",
    "    df['sender'] = user_label\n",
    "    # ensure timestamp if present\n",
    "    ts_cols = [c for c in df.columns if 'time' in c.lower() or 'timestamp' in c.lower()]\n",
    "    if len(ts_cols) > 0:\n",
    "        df['timestamp'] = pd.to_datetime(df[ts_cols[0]], errors='coerce')\n",
    "    else:\n",
    "        # create artificial timestamp using index (keep ordering)\n",
    "        df['timestamp'] = pd.NaT\n",
    "    # conversation id if exists\n",
    "    if 'conversation_id' not in df.columns and 'conv_id' in df.columns:\n",
    "        df = df.rename(columns={'conv_id': 'conversation_id'})\n",
    "    return df[['conversation_id'] if 'conversation_id' in df.columns else [] + ['timestamp', 'sender', 'message']]\n",
    "\n",
    "# read both\n",
    "userA = read_chat_file(\"C:/Users/heyja/Downloads/userA_chats.csv\", 'A')\n",
    "userB = read_chat_file('C:/Users/heyja/Downloads/userB_chats.csv', 'B')\n",
    "\n",
    "print('userA rows:', len(userA), 'userB rows:', len(userB))\n",
    "\n",
    "# if both files empty and xlsx exists, try reading it (user may have downloaded the sheet locally)\n",
    "if len(userA) == 0 and len(userB) == 0 and CONV_XLSX.exists():\n",
    "    try:\n",
    "        conv = pd.read_excel('C:/Users/heyja/Downloads/conversationfile.xlsx')\n",
    "        print('Read conversationfile.xlsx shape', conv.shape)\n",
    "        # try to detect sender column\n",
    "        if 'sender' in conv.columns and 'message' in conv.columns:\n",
    "            conversations = conv[['conversation_id'] if 'conversation_id' in conv.columns else [] + ['timestamp', 'sender', 'message']]\n",
    "        else:\n",
    "            # assume alternating two-person: tag alternately\n",
    "            conv = conv.reset_index(drop=True)\n",
    "            conv['sender'] = ['A' if i % 2 == 0 else 'B' for i in range(len(conv))]\n",
    "            conv['message'] = conv.iloc[:,0].astype(str)\n",
    "            conversations = conv[['timestamp', 'sender', 'message']]\n",
    "    except Exception as e:\n",
    "        print('Failed to read xlsx:', e)\n",
    "        conversations = pd.DataFrame()\n",
    "else:\n",
    "    # combine and sort\n",
    "    combined = pd.concat([userA, userB], ignore_index=True, sort=False)\n",
    "    # if timestamp present use it; else keep original ordering\n",
    "    if combined['timestamp'].notna().any():\n",
    "        combined = combined.sort_values('timestamp').reset_index(drop=True)\n",
    "    else:\n",
    "        combined = combined.reset_index(drop=True)\n",
    "    conversations = combined[['timestamp', 'sender', 'message']]\n",
    "\n",
    "print('Constructed conversations dataframe shape:', conversations.shape)\n",
    "conversations.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad889677-0d31-440a-83fa-be3de4ed95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples built: 10\n",
      "Train examples: 10 Val examples: 10\n"
     ]
    }
   ],
   "source": [
    "def build_examples_from_timeline(df: pd.DataFrame, max_history_msgs=10):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    examples = []\n",
    "    # iterate over timeline, find pattern B -> A\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.loc[i-1]\n",
    "        cur = df.loc[i]\n",
    "        if prev['sender'] == 'B' and cur['sender'] == 'A':\n",
    "            # collect previous A messages before position i-1 (history)\n",
    "            history_msgs = []\n",
    "            j = i-2\n",
    "            while j >= 0 and len(history_msgs) < max_history_msgs:\n",
    "                if df.loc[j]['sender'] == 'A':\n",
    "                    history_msgs.append(clean_text(str(df.loc[j]['message'])))\n",
    "                j -= 1\n",
    "            history_msgs = list(reversed(history_msgs))\n",
    "            examples.append({\n",
    "                'a_history': ' \\n '.join(history_msgs),\n",
    "                'b_message': clean_text(str(prev['message'])),\n",
    "                'a_reply': clean_text(str(cur['message'])),\n",
    "            })\n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "examples = build_examples_from_timeline(conversations, max_history_msgs=8)\n",
    "print('Examples built:', len(examples))\n",
    "examples.sample(3)\n",
    "\n",
    "# if zero examples, try a fallback: pair alternate messages\n",
    "if len(examples) == 0 and len(conversations) > 1:\n",
    "    fallback = []\n",
    "    for i in range(0, len(conversations) - 1, 2):\n",
    "        a_msg = conversations.loc[i]['message']\n",
    "        b_msg = conversations.loc[i+1]['message']\n",
    "        fallback.append({'a_history': '', 'b_message': str(b_msg), 'a_reply': str(a_msg)})\n",
    "    examples = pd.DataFrame(fallback)\n",
    "    print('Fallback pairing created examples:', len(examples))\n",
    "\n",
    "# split train/val\n",
    "train_df, val_df = train_test_split(examples, test_size=0.1, random_state=SEED) if len(examples) > 10 else (examples, examples)\n",
    "print('Train examples:', len(train_df), 'Val examples:', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d75c851d-e1aa-4b4c-8a0c-3f1f9bf417ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: distilgpt2\n",
      "Model max length (config): 1024\n"
     ]
    }
   ],
   "source": [
    "print('Loading tokenizer and model:', MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# ensure tokenizer has a pad token (gpt2 doesn't by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# resize embeddings if we added pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(DEVICE)\n",
    "\n",
    "print('Model max length (config):', getattr(model.config, 'n_positions', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb60af68-67bd-4fd8-8495-6c5ddcadf935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 10\n"
     ]
    }
   ],
   "source": [
    "PREFIX_TEMPLATE = \"A_HISTORY: {a_history} \\n B: {b_message} \\n A:\"\n",
    "\n",
    "class ChatReplyDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_context_tokens=MAX_CONTEXT_TOKENS, max_reply_tokens=MAX_REPLY_TOKENS):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.max_reply_tokens = max_reply_tokens\n",
    "        for _, row in df.iterrows():\n",
    "            a_hist = row.get('a_history', '') or ''\n",
    "            b_msg = row.get('b_message', '') or ''\n",
    "            a_reply = row.get('a_reply', '') or ''\n",
    "            prompt = PREFIX_TEMPLATE.format(a_history=a_hist, b_message=b_msg)\n",
    "            # full text = prompt + ' ' + reply\n",
    "            full_text = prompt + ' ' + a_reply\n",
    "            # tokenize without truncation first (we will manually enforce)\n",
    "            enc = tokenizer(full_text, return_tensors='pt', truncation=False)\n",
    "            input_ids = enc['input_ids'][0]\n",
    "            # find where reply starts (by tokenizing prompt)\n",
    "            prompt_enc = tokenizer(prompt, return_tensors='pt', truncation=False)\n",
    "            prompt_len = prompt_enc['input_ids'].size(1)\n",
    "            # enforce length constraints from the right: keep last max_context_tokens of prompt tokens\n",
    "            # and the reply truncated to max_reply_tokens\n",
    "            # If prompt too long, truncate prompt tokens from the left (keep recent context)\n",
    "            # We'll compute token segments:\n",
    "            reply_ids = input_ids[prompt_len:]\n",
    "            prompt_ids = input_ids[:prompt_len]\n",
    "            # truncate reply\n",
    "            if reply_ids.size(0) > max_reply_tokens:\n",
    "                reply_ids = reply_ids[:max_reply_tokens]\n",
    "            # truncate prompt to keep recent tokens\n",
    "            if prompt_ids.size(0) > max_context_tokens:\n",
    "                prompt_ids = prompt_ids[-max_context_tokens:]\n",
    "            # compose final sequence\n",
    "            final_ids = torch.cat([prompt_ids, reply_ids], dim=0)\n",
    "            # labels: -100 for prompt tokens, actual ids for reply tokens\n",
    "            labels = final_ids.clone()\n",
    "            labels[:prompt_ids.size(0)] = -100\n",
    "            # pad to model max length if necessary\n",
    "            max_len = self.tokenizer.model_max_length\n",
    "            if final_ids.size(0) > max_len:\n",
    "                final_ids = final_ids[-max_len:]\n",
    "                labels = labels[-max_len:]\n",
    "            self.examples.append({'input_ids': final_ids, 'labels': labels})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.examples[idx]\n",
    "        return {k: v.clone().detach() for k, v in item.items()}\n",
    "\n",
    "# collate function to pad batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    labels = [b['labels'] for b in batch]\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    attention_mask = (input_ids_padded != tokenizer.pad_token_id).long()\n",
    "    return {'input_ids': input_ids_padded, 'attention_mask': attention_mask, 'labels': labels_padded}\n",
    "\n",
    "# build dataset objects\n",
    "train_dataset = ChatReplyDataset(train_df, tokenizer) if len(train_df) > 0 else None\n",
    "eval_dataset = ChatReplyDataset(val_df, tokenizer) if len(val_df) > 0 else None\n",
    "print('Train dataset size:', len(train_dataset) if train_dataset else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ee2e7f-1eea-4ad9-be58-2f0c935036d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import inspect\n",
    "print(inspect.getfile(transformers.TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4781bc2-87a2-4088-a011-18aba9952ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print([p for p in sys.path if 'transformers' in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0957673-e484-450e-bd94-dfba4517c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, bool] = <factory>, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86446e94-ceb9-4320-a2df-a3f0de70d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.698051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.687875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n",
      "Model and tokenizer saved to chatrec_output\\model\\final\n"
     ]
    }
   ],
   "source": [
    "if train_dataset is not None and len(train_dataset) > 0:\n",
    "    eval_strategy = 'epoch' if (eval_dataset is not None and len(eval_dataset) > 0) else 'no'\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / \"model\"),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    eval_strategy=\"epoch\" if eval_dataset is not None and len(eval_dataset) > 0 else \"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset if eval_dataset is not None and len(eval_dataset) > 0 else None,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    # quick sanity check run (optional):\n",
    "    print('Starting training...')\n",
    "    trainer.train()\n",
    "    print('Training finished')\n",
    "\n",
    "    # save\n",
    "    model_save_path = OUTPUT_DIR / 'model' / 'final'\n",
    "    model.save_pretrained(str(model_save_path))\n",
    "    tokenizer.save_pretrained(str(model_save_path))\n",
    "    print('Model and tokenizer saved to', model_save_path)\n",
    "else:\n",
    "    print('No training data found; skipping training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a76bc36-7412-4b96-8b4a-7bfefe545e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU: 0.0040  ROUGE-L: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heyja\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 3.6878750324249268 Perplexity: 39.95984330294441\n",
      "Saved generation results to chatrec_output\\val_generation_results.csv\n"
     ]
    }
   ],
   "source": [
    "def generate_reply(model, tokenizer, a_history: str, b_message: str, gen_kwargs=None):\n",
    "    if gen_kwargs is None:\n",
    "        gen_kwargs = {}\n",
    "    prompt = PREFIX_TEMPLATE.format(a_history=a_history, b_message=b_message) + ' '\n",
    "    # ensure prompt fits model\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length - 1).input_ids.to(DEVICE)\n",
    "    # generate\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=min(tokenizer.model_max_length, input_ids.shape[1] + GEN_MAX_LENGTH),\n",
    "        do_sample=True,\n",
    "        temperature=GEN_TEMPERATURE,\n",
    "        top_k=GEN_TOP_K,\n",
    "        top_p=GEN_TOP_P,\n",
    "        num_return_sequences=NUM_RETURN_SEQUENCES,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    # decode the completed sequence and return only the part after the prompt\n",
    "    generated = []\n",
    "    for out in outputs:\n",
    "        out_text = tokenizer.decode(out[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        generated.append(out_text.strip())\n",
    "    return generated\n",
    "\n",
    "# Evaluate on validation set (if exists)\n",
    "if eval_dataset is not None and len(eval_dataset) > 0:\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    for idx, row in val_df.reset_index(drop=True).iterrows():\n",
    "        a_hist = row['a_history']\n",
    "        b_msg = row['b_message']\n",
    "        ref = row['a_reply']\n",
    "        gen = generate_reply(model, tokenizer, a_hist, b_msg)[0]\n",
    "        refs.append(ref)\n",
    "        hyps.append(gen)\n",
    "        # BLEU (sentence-level small smoothing)\n",
    "        try:\n",
    "            bleu = sentence_bleu([nltk.word_tokenize(ref.lower())], nltk.word_tokenize(gen.lower()), smoothing_function=SmoothingFunction().method1)\n",
    "        except Exception:\n",
    "            bleu = 0.0\n",
    "        bleu_scores.append(bleu)\n",
    "        rouge_scores.append(rouge_l_score(ref, gen))\n",
    "    avg_bleu = float(np.mean(bleu_scores))\n",
    "    avg_rouge_l = float(np.mean(rouge_scores))\n",
    "    print(f'Validation BLEU: {avg_bleu:.4f}  ROUGE-L: {avg_rouge_l:.4f}')\n",
    "    # compute perplexity via trainer.evaluate if possible\n",
    "    try:\n",
    "        eval_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "        eval_loss = eval_metrics.get('eval_loss')\n",
    "        perplexity = math.exp(eval_loss) if eval_loss is not None and eval_loss < 100 else float('inf')\n",
    "        print('Eval loss:', eval_loss, 'Perplexity:', perplexity)\n",
    "    except Exception as e:\n",
    "        print('Could not compute perplexity via trainer:', e)\n",
    "\n",
    "    # save a small csv with results\n",
    "    res_df = pd.DataFrame({'reference': refs, 'generated': hyps, 'bleu': bleu_scores, 'rouge_l': rouge_scores})\n",
    "    res_df.to_csv(OUTPUT_DIR / 'val_generation_results.csv', index=False)\n",
    "    print('Saved generation results to', OUTPUT_DIR / 'val_generation_results.csv')\n",
    "else:\n",
    "    print('No eval dataset - skipping generation evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14dfcb28-b1fa-435e-a586-0b0cfa6c9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.joblib to chatrec_output\\Model.joblib\n"
     ]
    }
   ],
   "source": [
    "class ChatRecPredictor:\n",
    "    def __init__(self, model_dir: str):\n",
    "        self.model_dir = model_dir\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "        self.device = DEVICE\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, a_history: str, b_message: str, num_return_sequences=1):\n",
    "        prompt = PREFIX_TEMPLATE.format(a_history=a_history, b_message=b_message) + ' '\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=self.tokenizer.model_max_length - 1).input_ids.to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=min(self.tokenizer.model_max_length, input_ids.shape[1] + GEN_MAX_LENGTH),\n",
    "            do_sample=True,\n",
    "            temperature=GEN_TEMPERATURE,\n",
    "            top_k=GEN_TOP_K,\n",
    "            top_p=GEN_TOP_P,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        results = []\n",
    "        for out in outputs:\n",
    "            out_text = self.tokenizer.decode(out[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            results.append(out_text.strip())\n",
    "        return results\n",
    "\n",
    "# if we saved model above, create predictor and joblib dump\n",
    "model_dir = str((OUTPUT_DIR / 'model' / 'final'))\n",
    "if os.path.exists(model_dir):\n",
    "    predictor = ChatRecPredictor(model_dir)\n",
    "    joblib.dump(predictor, OUTPUT_DIR / 'Model.joblib')\n",
    "    print('Saved Model.joblib to', OUTPUT_DIR / 'Model.joblib')\n",
    "else:\n",
    "    # if model not saved (training skipped), save a small metadata joblib instructing user how to load\n",
    "    meta = {'note': 'No trained model found. Run training cells first and save model to ./chatrec_output/model/final'}\n",
    "    joblib.dump(meta, OUTPUT_DIR / 'Model.joblib')\n",
    "    print('Saved placeholder Model.joblib to', OUTPUT_DIR / 'Model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b6149-4de0-4132-bb93-ab0141e9546e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
